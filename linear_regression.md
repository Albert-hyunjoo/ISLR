# Linear Regression

* `선형 회귀`는 통계학에서 가장 기본적으로 사용되는 `양적 변수`의 회귀법
* 많은 대부분의 최신 통계학적 기법이 이 `선형 회귀`를 기반으로 만들어진 경우가 많다.
* `LSE (Least Square)`는 `선형 회귀`의 가장 대표적인 통계학적 척도이다.

> 선형 회귀를 통해 어떤 부분을 체크해야 하는가?
> * (양적) **변수**들 사이에 실제로 **관계가 존재**하는가?
> * 관계가 있다면, 얼마나 강력한가? (상관계수)
> * 어떤 변수가 실제로 종속변수에 영향을 미치는가?
> * 어떻게 각 변수가 끼치는 영향을 측정할 수 있는가?
> * 나중에 얼마나 정확하게 미래 변수의 변화를 예측할 수 있는가?
> * 관계는 선형적인가?
> * 다른 변수들 사이에 시너지가 존재하는가?

## Simple Linear Regression
* **간단한 선형 회귀**는 양적 변수 `X`와 `Y` 사이의 간단한 관계이다.
* 이 경우에는 `Y ⩬ β₀ (파라미터 1)+ β₁ (파라미터 2) X` 형태로 표현한다.
* 이러한 관계를 `훈련 데이터`를 통해 훈련시켜서 `실제 데이터 분석 및 예측`에 응용한다.

### Estimating The Coefficient
* 어떠한 `선형 모델`을 파악할 때, 실제 식과 예측식 사이의 **closeness**를 구해야 한다.
* 이를 측정하는 가장 대표적인 방법은 `Minimizing The Least Square` 이다.
* `(^Y) ⩬ (^β₀) + (^β₁)Xi`라는 예측값과 참값 (`Y`) 사이의 차이를 `Ei`라고 한다면    
  `RSS`는 `E₁²+E₂²...`으로 표현될 수 있다.
* 이 `RSS`가 최소가 되는 (`^β₁`)와 (`^β₀`)은 다음과 같다:    
    > `^β₁` = [Σ(n, i=1)(Xi-x_bar)*(Yi-y_bar)]/[Σ(n, i=1)(Xi-X_bar)²]    
    `^β₀` = y_bar -  `^β₁`x_bar
* 실제적인 관계의 경우에는 예상과 다르게 `선형관계`가 아닐 수도 있다.

### Assessing The Accuracy of The Coefficient Estimates
* 보통 통상적으로 안에 있는 여러 요인에 의해서 다음의 상황이 만족된다.
> Y ⩬ β₀ (파라미터 1)+ β₁ (파라미터 2) X + e
> * 여기서 그 e의 평균은 0에 해당한다.
* 우리는 어떤 `데이터 모집단에 대한 정보 (μ, δ)` 를 알 수 없다.
* 대신, `샘플의 평균 (y_bar)`를 `이를 추정하기 위한 수단`으로 활용할 수 있다.
* **왜?** 어떤 샘플의 평균은 모평균을 **과대**, 어떤 곳은 **과소평가**하지만 평균은 결국 **모평균**이다.
* 단, **하나의 표본평균**은 분명 **모평균**과 차이가 생기는데, 이는 `Standard Error (SE)`라고 한다.
> 이를 식으로 표현하면 다음과 같다:    
> 1) `Var(^μ)` = SE(^μ)^2 = δ^2/n
> 2) SE(`^β₁`)^2 = δ^2 [(1/n) + (x_bar^2/Σ(n, i=1)(Xi-x_bar)^2)]
> 3) SE(`^β₀`)^2 = δ^2 / Σ(n, i=1)(Xi-x_bar)^2    
> * 여기서 δ^2 = Var(e) (단, 에러가 δ^2에 영향 X)
> * 만약 `x_bar` = 0이면, SE(`^β₀`) = SE(`μ_hat`)
* 이렇게 구한 `Standard Error (SE)`는 `신뢰구간` 및 `가설검정`에 사용된다.
* 가설검정에는 `t-통계량`을 사용, 통상적으로 `n = 30이상`은 정규분포와 유사하게 근사한다.
* 이를 통해 얻는 `p-value`는 곧 변수 `X`와 `Y`간의 **관계가 우연이라고 보기는 어렵다**는 뜻이다.

### Assessing the Accuracy of The Model
* 가설검정을 통해 모델을 밝혀냈으면, 이제는 그 `모델의 정확도`를 파악해야 한다.
* 이를 측정할 수 있는 대표적인 방법은 `RSE (Residual Std. Error)`와 `R^2` 이다.

#### Residual Standard Error
* `RSE`는 `e`의 표준편차의 평균애 해당되며, `response`가 `True Regression Line` 사이의 차이의 평균
> `RSE` = √[(1/n-2) * `RSS`] = √[(1/n-2) * Σ(n~i=1)(Yi-^Yi)^2]

#### R^2
* `RSE`의 문제는 이 단위가 y이므로 이를 비교하기 적절치 않다는 점이다.
* 이를 보완하기 위해 비율 형태로 `RSE`를 표현하는 것의 단위가 바로 `R^2`이다.
> `R^2` = (`TSS`-`RSS`)/`TSS` = 1 – `RSS`/`TSS`    
> (여기서 `TSS`는 `Total Sum of Square`로 `Σ(n~i=1)(Yi-y_bar)^2` 이다.)
* 단순한 선형 회귀 하에서는, `R^2 = r^2`가 성립이 된다.

## Multiple Linear Regression
* `다중 선형 회귀`는 단순한 일반적 선형 회귀 그 이상에 해당한다.
* 변수 사이의 시너지 등을 고려했을 때, `다중 선형 회귀`는 다음의 식으로 확장한다.
> Y = β₀ + β₁X₁ + β₂X₂ + β₃X₃ .... + e

### Estimating the Regression Coefficients
* 통상적으로 우리는 다중 선형 회귀식의 정확한 계수를 알 수 없으므로 어림해야 한다.
> ^Y = ^β₀ + ^β₁X₁ + ^β₂X₂ + ^β₃X₃ ....
* 다중 회귀 상의 `RSS`는 다음과 같이 구할 수 있다.
> `RSS` = Σ(i=1~n)*(Yi – ^Yi)^2
* 단순 선형 회귀와 달리, **다중 선형 회귀**를 하면서 **이전의 변수**가 다른 영향을 끼칠 수도 있다.

### Some Important Questions
* 다중 선형 회귀를 시행할 때는 다음과 같은 질문에 주목해야 한다.
> 1) 1개 이상의 변수가 응답의 예측에 도움이 되는가?
> 2) 변수들 중 모든 변수 or 그 중 일부가 도움이 되는가?
> 3) 모델이 얼마나 데이터에 맞는가?
> 4) 여러 변수값이 있을 때, 어떤 response value를 예측하고 이는 얼마나 정확한가?

#### 응답과 예측자의 관계
* `검정 통계`를 통해서 실제로 예측자가 끼치는 영향을 주목한다.
> f-통계의 귀무가설 및 대립가설 :    
> H0 : β₁ = β₂ = β₃.... = 0    
> HA : 적어도 하나의 β가 0이 아니다.    
> **F-통계량**의 값 = ((`TSS`-`RSS`)/p) / (`RSS`* (n-p-1))    
> *만약 f-통계량이 1보다 높으면 이는 귀무가설 기각의 근거가 된다.*    
> *만약 n의 값이 높으면, 1보다 약간만 높아도 귀무가설 기각의 근거가 될 수 있다.*    
> *H0이 참이고 에러가 정규분포를 따르면, 이 통계량은 f-분포를 따른다*
* `Partial Effect`의 검증을 위해 subset인 `q`를 기준으로 `F-test`를 실시하기도 한다.
* `F-value`는 예측자나 관찰자의 갯수에 영향을 받지 않는다. (단, p가 작고 n도 작을 때)
* 만약 p가 n보다 많으면, `forward selection` 등의 alternative를 사용해야 한다.
> **F-통계량**의 값 = ((`RSS₀`-`RSS`)/p) / (`RSS`* (n-p-1))   

#### 중요한 변수의 결정
* F-통계로 변수가 회귀에 관계 있음을 증명한 다음에는, 어떤 변수인지를 찾는 것!
* 통계학적으로 회귀에 영향이 있는 변수를 찾는 과정을 `Variable Selection`이라 한다.
* 이를 측정하는 척도는 `AIC`, `BIC`, `Adjusted-R^2`가 있다.
* 변수를 선택해나가는 과정은 크게 **3가지 방법**이 존재한다.
    1) Forward: null model에 p를 하나씩 더해가는 방법
    2) Backward: 모든 변수가 포함된 모델에서 하나씩 빼는 방법
    3) Mixed Selection: 두개를 섞어서 쓰는 방법

#### Model Fit
* 어떤 모델 적합도를 측정하는 척도는 `R^2`와 `RSE`에 해당한다.
* 다중 변수 회귀 환경 하에서 `R^2`는 `Cor(Y, ^Y)^2`, 즉 `Response`와 `Fitted Linear Model`의 상관관계 제곱이다.
* `Fitted Model`은 **이 수치가 가장 높은 환경**임을 의미한다.
* 즉, `R^2`가 1에 가까울 수록 이 모델이 **데이터의 대부분을 설명할 수 있다**는 의미이다.
* 단, 변수를 넣었을 때 그 수치에 큰 변화가 없으면 이에 대해 신중해야 한다.
* `RSE`는 **√(1/n-p-1 * `RSS`)**로, 만약 변수 (p) 가 (RSS와 비교) 많아지면 `RSE`가 늘어난다.

#### Prediction
* 예측에는 다음의 **3가지 형태의 불확실성**이 존재한다.
  1) True Model인 `f(X) = β₀ + β₁X₁ + β₂X₂ + β₃X₃ ....`과      
     Regression Model인 `^Y = ^β₀ + ^β₁X₁ + ^β₂X₂ + ^β₃X₃ ....` 사이의 차이
  * 이 경우에는 Confidence Interval로 계산해서 그 정도를 구한다.
  2) Regression Model 자체에 존재하는 `model bias`가 존재한다.
  3) 설령 True Model을 알아도, `random error (e)`가 여전히 존재한다.
  * 이 경우에는 Prediction Interval로 예측에 사용한다.
  
### Other Considerations

#### Qualitative Predictors (2개)
* 만약 고려해야 할 변수가 질적 변수라면 다른 `Regression` 방법을 보아야 한다.
* 만약 Predictor가 레벨이 2개면 `Dummy Variable`을 만들어서 `Regression`을 한다.

#### Qualitative Predictors (2+)
* 2개 이상의 질적 변수가 존재한다면, 이것도 `Dummy Variable`을 생성하는 식으로 진행한다.
* 단, 그 변수들의 개수는 **레벨 수에 비해 하나가 작은데**, 이는 `Baseline`이다.
* 회귀를 한 다음에 `p-value`를 측정하고, `F-test`를 통해서 이 `coefficient`가 통계적으로 의미가 있는지를 알 수 있다.     
*데이터가 샘플링된 모집단에 가장 적합한 모델을 식별하기 위해 데이터 세트에 맞는 통계 모델을 비교할 때 가장 자주 사용된다.*

### Extension of The Linear Model
* 일반적인 **선형 회귀**는 두 변수 사이의 관계가 `additive`하고  `linear`함을 전제로 한다.
* `additive`는 독립 변수인 `Xj`에 대해 `Y`의 변화가 **독립적**으로 움직인다는 것이며,
* `linear`는 `Xj`의 유닛당 변동 **정도가 일정**하다는 의미이다.

#### Removing Additive Effect
* 만약 한 변수가 다른 변수와 `Synergy`가 보이면, 이는 올바른 선형 회귀식을 도출하지 않는다.
* 즉, `β₀X1 + β₁X2..` 대신 `β₀X1X2 + β₁X2` 이런 식으로 얽힘 효과가 보인다.
* 이런 시너지가 존재하는 식과 **별도의 식**을 비교해서 **그 효과의 통계적 영향**을 확인할 수 있다.
* 가끔씩 어떤 시너지의 계수의 `p-value`가 높지 않을 때도 존재하는데,     
  `hiearchial principle`에 따라 `시너지`가 들어갔으면 `main effect`도 함께 넣어야 한다.
  
#### Non-Linear Relationships
* 만약 두 변수 사이의 관계가 선형 관계가 아니면 선형 회귀는 쓸모가 없다.
* 대표적인 사례가 `다항식적 회귀 모델` 인데, 이 경우 대사 변수가 *제곱을 하는 식*으로 표현한다.
